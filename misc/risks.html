<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN">
<html>
<head>
  <title>risks</title>
  	<link rel="stylesheet" href="../css/mystyles.css">
  </head>

<body>
  <h1 class="banner">Group 0314</h1>
<!-- Site navigation menu -->

<ul class="navbar">
  <li><a href="../index.html">Home page</a>
  <li><a href="topic.html">Technology/Topic</a>
	<li><a href="opportunities.html">Opportunities</a>
	<li><a href="misc/risks.html">Risks</a>
	<li><a href="choices.html">Choices</a>
	<li><a href="ethics.html">Ethical Reflections</a>
	<li><a href="references.html">References</a>
   <li><a href="process.html">Process Support</a>
</ul>

<!-- Main content -->
<h1>Technology Risks</h1>


</ul><h4><p>The page or area describing the risks posed by your chosen technology/topic.<p></h4>

<h3><em>Artifical Intelligence working in Construction.</em></h3>
<p><em>A potential issue that we may encounter would be the AI’s lack of understanding towards the human workers. 
  The AI cannot determine the pace of a contractor's average working speed without studying that individual during a project. 
  Nor will the AI be able to have a strong understanding of the level of tiredness or level of stress that the team by be experiencing. 
  Sinz (2019) seconds this when saying “The lack of robustness to simple changes in the input statistics indicates that deep networks lack human-like scenes of understanding”.
   (vol. 103). Although having a system like this can save companies a lot of money in the long run, the initial startup cost would be substantial compared to hiring an experienced worker. 
   But as previously stated in our introduction to our topic. </em></p>


<p><em>AI working for us in the workforce is a bittersweet idea. Helping to prevent loss of money and major accidents is a huge advantage. 
  However, this will take away jobs away from society, and in a world where people are already struggling to find work, it is not ideal. 
  Another reason why we should not implement ideas like this is because AI has not reached that level of intelligence yet. If we cannot perform a certain task, 
  how are we expected to teach a computer how to do it? This is also shown in writing by Jordan, M. I. (2019). According to Jordan, M. I. 
  (2019) “human intelligence is the only kind of intelligence we know; thus, we should aim to mimic it as a first step. However, 
  humans are in fact not very good at some kinds of reasoning— we have our lapses, biases, and limitations. Moreover, critically, 
  we did not evolve to perform the kinds of large-scale decision-making that modern II systems must face, nor to cope with the kinds of uncertainty that arise in II contexts”. </em></p>

  <h3><em>Artificial Intelligence in medical applications.</em></h3>
  <em>
  <p>The integration of AI systems into the medical setting brings about significant benefits, but it also entails risks related to patient safety. These risks primarily arise from the potential for incorrect or inaccurate information generated by AI algorithms, which can have serious consequences, including injury or even death in extreme cases. One of the primary concerns is the reliability of AI-generated diagnoses and treatment recommendations. While AI algorithms can process vast amounts of medical data and identify patterns, there is still a possibility of errors or misinterpretations. Inaccurate diagnoses may lead to inappropriate treatments or delayed interventions, negatively impacting patient outcomes. Therefore, it is crucial to rigorously evaluate and validate AI algorithms before their deployment in real-world healthcare settings.</p>
  <p>Another risk is the potential for biases in AI systems. “In the healthcare context, biased AI models may overestimate or underestimate health risks in specific patient populations. For instance, AI systems may engage in stereotyping and exhibit gender or racial bias. Bias in AI models my also occur when datasets are not representative of the target population, or incomplete and inaccurate data are used by AI systems for decision-making.” (Esmaeilzadeh, 2020). If the training data used to develop AI algorithms is biased or unrepresentative, the generated recommendations may be skewed or discriminatory. Biases can disproportionately affect certain patient populations, leading to disparities in healthcare delivery. Addressing bias in AI systems requires careful data curation, diverse representation in training datasets, and ongoing monitoring and mitigation of bias throughout the AI lifecycle.</p>
  <p>Additionally, the reliance on AI systems in critical medical decision-making raises concerns about the explain-ability and transparency of these systems. When AI algorithms generate diagnoses or treatment plans, it is essential for healthcare professionals to understand the underlying reasoning and justifications. Lack of explain-ability can erode trust and confidence in AI systems, making it challenging for doctors to make informed decisions or verify the accuracy of AI-generated information.</p>
  </em>
	
	
	<h3><em>Cost to run Artificial Intelligence in the future. </em></h3> 
	<p><em>We must acknowledge that AI has not reached its peak performance yet. Many human factors must be put into place for Ai to use itself in the long run. 
		Tesla’s cars cannot run without AI because of its safety features and hands-on driving experience. (Crawford, k.,2021).
		A lot of human factors such as making the software for the ai to run on and constantly updating it to work well with the modern world are needed to appreciate and apprehend the fact that ai can be used appropriately. 
		A lot of money has to b spent to make AI useful in certain scenarios and situations. Apple’s “Siri” AI software cannot be used to make the same type of software as a burger menu at Mcdonalds, therefore, 
		costing companies a lot of money to make different software for different situations. It is considered that, if AI can take the best possible action toward a goal, it can be trusted to make high-stakes decisions in health, education, and criminal justice (Crawford, k.,2021).
		Studies from Bukhoree, S (2023) and Anant, C (2023) showed that they used multiple Ai based deep learning systems in an emergency to let it decide on a high stakes outcome. 
		None of the AI systems considered the fact or need of human interpretation therefore leading to a less cognitive outcome (Suhoh and Choksuriwong., 2023).  </em></p>


<!-- Sign and date the page, it's only polite! -->
<address>Made 1 March 2021<br>
  by Tony Clear.</address>
 
<p><em>thanks to W3C for tutorial and adapted code from <a href="https://www.w3.org/Style/Examples/011/firstcss.en.html">Style Examples</a></p></em>
<p><em>also thanks to WDN for HTML and CSS resources and any adapted code snippets from <a href= "https://developer.mozilla.org/en-US/docs/Web">Mozilla Developer Network</a></p></em>
 </html>
